{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a3a23e",
   "metadata": {},
   "source": [
    "# Time Series Regression Analysis (Corporation Favorita)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7fd35a",
   "metadata": {},
   "source": [
    "## `Business Understanding`\n",
    "**Project Scenario**\n",
    "\n",
    "You are a data scientist in Corporation Favorita, a large Ecuadorian-based grocery retailer. Corporation Favorita wants to ensure that they always have the right quantity of products in stock. To do this you have decided to build a series of machine learning models to forecast the demand of products in various locations. The marketing and sales team have provided you with some data to aid this endeavor. Your team uses CRISP-DM Framework for Data Science projects\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "Corporation Favorita aims to optimize its inventory management by accurately forecasting the demand for various products across its stores in Ecuador. The goal is to ensure that each store has the right quantity of products in stock to meet customer demand while minimizing overstocking or stockouts\n",
    "    \n",
    "**Objective**\n",
    "\n",
    "The objective is to build machine learning models that can predict unit sales for different product families at Favorita stores accurately. These models will help optimize inventory levels, improve sales forecasting accuracy, and ultimately enhance customer satisfaction by ensuring product availability.\n",
    "\n",
    "**Key Stakeholders**\n",
    "\n",
    "Stakeholders include Corporation Favorita's management, sales and marketing teams, store managers, and data science team.\n",
    "\n",
    "**Analytical Goals**\n",
    "- Handle missing values in the datasets by imputation techniques such as mean, median, or mode.\n",
    "- Address outliers in sales data that may skew the model's predictions by applying robust statistical methods or trimming techniques.\n",
    "- Normalize or scale numerical features to ensure uniformity and improve model performance.\n",
    "- Encode categorical variables using techniques such as one-hot encoding or label encoding.\n",
    "- Build time series regression models such as SARIMA, ARIMA, XGBoost, Linear Regression etc. to capture seasonality and trends in sales data.\n",
    "- Validate models using cross-validation techniques and assess their performance metrics such as RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error).\n",
    "- Create insightful visualizations and dashboards for sales analysis and forecasting.\n",
    "\n",
    "**Success Criteria**\n",
    "- Achieve a 0.2 RMSE (Root Mean Squared Error) in sales forecasting models.\n",
    "- Improve inventory management efficiency and reduce stockout instances.\n",
    "\n",
    "**Constraints and Assumptions**\n",
    "- Assumption: Historical sales data is representative of future demand patterns.\n",
    "- Constraint: Limited availability of real-time sales data for model training.\n",
    "\n",
    "**Data Requirements**\n",
    "- Utilize data from train.csv, stores.csv, holidays_events.csv, oil.csv, and transaction.csv for analysis.\n",
    "- Include features such as store_nbr, family, onpromotion, store metadata, oil prices, holidays, and transactional data.\n",
    "\n",
    "**Business Impact**\n",
    "- Enhance customer satisfaction through better product availability.\n",
    "- Optimize inventory management, leading to cost savings and improved operational efficiency.\n",
    "\n",
    "### `Hypothesis`\n",
    "Null Hypothesis (Ho): Holidays do not have a significant effect on the sales \n",
    "\n",
    "Alternate Hypothesis (Ha): Holidays have a significant effect on the sales\n",
    "\n",
    "**Analytical Business Questions**\n",
    "\n",
    "1. Is the train dataset complete (has all the required dates)?\n",
    "2. Which dates have the lowest and highest sales for each year (excluding days the store was closed)?\n",
    "3. Compare the sales for each month across the years and determine which month of which year had the highest sales.\n",
    "4. Did the earthquake impact sales?\n",
    "5. Are certain stores or groups of stores selling more products? (Cluster, city, state, type)\n",
    "6. Are sales affected by promotions, oil prices and holidays?\n",
    "7. What analysis can we get from the date and its extractable features?\n",
    "8. Which product family and stores did the promotions affect.\n",
    "9. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "10. Does the payment of wages in the public sector on the 15th and last days of the month influence the store sales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407581a",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data Manupilation Packages\n",
    "from dotenv import dotenv_values\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Import Visualization packages\n",
    "import seaborn as sns\n",
    "import seaborn_polars as snl\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import hvplot.polars # noqa\n",
    "\n",
    "# Import stats packages\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Utility Packages:\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ðŸ›¬ Imported all packages.\", \"Warnings hidden. ðŸ‘»\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802468e8",
   "metadata": {},
   "source": [
    "## `Data Understanding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef57cd",
   "metadata": {},
   "source": [
    "#### `First Datasets from Database`\n",
    "I'm using Python's dotenv with a .env file to safely fetch the first datasets from a SQL database into my notebook. This keeps my database credentials private while allowing easy access to the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection string\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};MARS_Connection=yes;MinProtocolVersion=TLSv1.2;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the connect method of pyodbc library and pass in the connection string\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112328eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql query to get oil data\n",
    "query1 = \"SELECT * FROM dbo.oil\"\n",
    "oil_df = pl.read_database(query1, connection)\n",
    "\n",
    "# sql query to get holidays_events\n",
    "query2 = \"SELECT * FROM dbo.holidays_events\"\n",
    "holiday_events_df =  pl.read_database(query2, connection)\n",
    "\n",
    "#sql query to get stores\n",
    "query3 = \"SELECT * FROM dbo.stores\"\n",
    "stores_df =  pl.read_database(query3, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef448732",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c0cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(df, date_col):\n",
    "    # Convert date column to date type\n",
    "    df = df.with_columns(df[date_col].cast(pl.Date))\n",
    "    \n",
    "    # Sort the dataframe using the date column\n",
    "    df = df.sort(date_col)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d356c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to date type\n",
    "oil_df = to_date(oil_df, 'date')\n",
    "oil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df.glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc773f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_events_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime type\n",
    "holiday_events_df = to_date(holiday_events_df, 'date')\n",
    "holiday_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036944cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_events_df.glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_events_df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87495ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df.glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b35569",
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09db8ad7",
   "metadata": {},
   "source": [
    "`Second Datasets from Github`\n",
    "\n",
    "I obtained the second datasets from a GitHub repository, and I'll use Pandas to import the CSV file into my notebook for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train Data\n",
    "train_df = pl.read_csv('Data/train.csv')\n",
    "\n",
    "# Loading the transaction data\n",
    "transactions_df = pl.read_csv('Data/transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to date type\n",
    "transactions_df = to_date(transactions_df, 'date')\n",
    "transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d752c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8319390",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171828a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad510ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column in train df to date type\n",
    "train_df = to_date(train_df, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the train data frame\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7032cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec28d1",
   "metadata": {},
   "source": [
    "### **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the descriptive statistics of the train data set\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cc1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking  for unique values in all the columns\n",
    "cols = train_df.columns\n",
    "\n",
    "results = []\n",
    "\n",
    "for col in cols:\n",
    "    \n",
    "    unique_values = train_df[col].unique()\n",
    "    num_unique_values = train_df[col].n_unique()\n",
    "    results.append([col, unique_values, num_unique_values])\n",
    "\n",
    "results_df = pl.DataFrame(results, schema=['Column', 'Unique_Values', 'Num_Unique_Values'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b040f4da",
   "metadata": {},
   "source": [
    "Checking for Data Completness and Missing dates in our Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1ec16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the data using the date \n",
    "start_date = train_df['date'].min()\n",
    "\n",
    "# End of the data using the date\n",
    "end_date = train_df['date'].max()\n",
    "\n",
    "# Print the start and end date of the data\n",
    "print(f'Start date is {start_date}')\n",
    "print(f'End date is {end_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_dates(df):\n",
    "    # Getting the date range for the train data set\n",
    "    date_range = pl.date_range(start = start_date, end = end_date, interval = '1d', eager = True)\n",
    "    \n",
    "    # Getting the existing dates\n",
    "    existing_dates = df['date']\n",
    "\n",
    "    # Getting the missing dates using the date range and the existing dates\n",
    "    missing_dates = date_range.filter(~date_range.is_in(existing_dates))\n",
    "    \n",
    "    return missing_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9549daf",
   "metadata": {},
   "source": [
    "- Check missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454562b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing dates\n",
    "all_missing_dates = missing_dates(train_df)\n",
    "all_missing_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a36d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Id column since it will not be relevant for our visualizations\n",
    "train_df = train_df.drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9607606",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f1a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to fill\n",
    "columns = [column for column in train_df.columns if column != 'date']\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1916da",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_enteries_per_day = (train_df.select(pl.col('date').filter(date=pl.col('date').min()))).shape[0]\n",
    "no_enteries_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de36655",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_enteries_per_day * len(all_missing_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2417e48",
   "metadata": {},
   "source": [
    "- Create missing dataframe using all missing dates, unique store_nbr and unique family category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df with unique values for store_nbr\n",
    "store_nbr_unique = train_df['store_nbr'].unique()\n",
    "\n",
    "# Create df with unique values for family category\n",
    "family_unique = train_df['family'].unique()\n",
    "\n",
    "# Create dataframe with missinng dates and unique store_nbr and family category\n",
    "missing_df = pl.DataFrame(\n",
    "    list(product(all_missing_dates, store_nbr_unique, family_unique)), \n",
    "    schema=['date', 'store_nbr', 'family']\n",
    ")\n",
    "\n",
    "missing_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa8765",
   "metadata": {},
   "source": [
    "- Add sales and onpromotion columns to the missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = missing_df.with_columns(\n",
    "    sales=None,\n",
    "    onpromotion=None,\n",
    ")\n",
    "\n",
    "missing_df.glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdddf24",
   "metadata": {},
   "source": [
    "- Join missing_df to the train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9845c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join original train_df with missing_df\n",
    "train_df = train_df.vstack(missing_df).sort('date')\n",
    "train_df.glimpse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1a98d",
   "metadata": {},
   "source": [
    "- Again, check missing dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb69f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing dates\n",
    "all_missing_dates = missing_dates(train_df)\n",
    "all_missing_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79787ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58572f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ed619",
   "metadata": {},
   "source": [
    "- Fill missing values in sales with 0.0 and onpromotion with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill sales with zero strategy\n",
    "train_df = train_df.fill_null(strategy=\"zero\")\n",
    "\n",
    "train_df.glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b062832",
   "metadata": {},
   "source": [
    "- Now, there are no missing values in sales and onpromotion columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.null_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c2453",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year from the date and adding it to the df as a new column\n",
    "train_df = train_df.with_columns(pl.col('date').dt.year().alias('year'))\n",
    "\n",
    "# Extracting the month from the date and adding it to the df as a new column\n",
    "train_df = train_df.with_columns(pl.col('date').dt.month().alias('month'))\n",
    "\n",
    "# Extracting the day from the date and adding it to the df as a new column\n",
    "train_df = train_df.with_columns(pl.col('date').dt.day().alias('day'))\n",
    "\n",
    "# Preview of the train Data frame\n",
    "train_df.glimpse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d395b7",
   "metadata": {},
   "source": [
    "### **Univariate Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c77ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.glimpse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3caa858",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('family').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fbb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "snl.boxplot(train_df.drop(['year', 'sales', 'onpromotion', 'family']), orient = 'h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to keep for the boxplot\n",
    "columns_to_keep = [col for col in train_df.columns if col not in ['year', 'sales', 'onpromotion', 'family']]\n",
    "df_for_boxplot = train_df.select(columns_to_keep)\n",
    "\n",
    "# Plotting the boxplot\n",
    "plt.figure(figsize=(12, 5))\n",
    "snl.boxplot(df_for_boxplot, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "snl.boxplot(train_df['year'], orient = 'h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "snl.boxplot(train_df['onpromotion'], orient = 'h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a6871",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "snl.boxplot(train_df['sales'], orient = 'h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d154c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaecc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "snl.kdeplot(train_df.drop(columns=['year', 'sales']), alpha=0.3, fill=True)\n",
    "plt.title('Density Plot for Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def7ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "snl.kdeplot(train_df['year'], alpha=0.3, fill=True)\n",
    "plt.title('Density Plot for year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "snl.kdeplot(train_df['sales'], alpha=0.3, fill=True)\n",
    "plt.title('Density Plot for Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_skew = train_df['sales'].skew()\n",
    "onpromotion_skew = train_df['onpromotion'].skew()\n",
    "store_skew = train_df['store_nbr'].skew()\n",
    "date_skew = train_df['date'].skew()\n",
    "\n",
    "print(f'Sales skewness = {sales_skew}')\n",
    "print(f'onpromotion skewness = {onpromotion_skew}')\n",
    "print(f'Store skewness = {store_skew}')\n",
    "print(f'Date skewness = {date_skew}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.plot(train_df['date'], train_df['sales'], marker='o', linestyle='-', color='b')  # Plotting sales against date\n",
    "plt.xlabel('Date')  # Label for the x-axis\n",
    "plt.ylabel('Sales')  # Label for the y-axis\n",
    "plt.title('Sales Over Time')  # Title for the plot\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.grid(True)  # Enable grid lines\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4)) \n",
    "sns.lineplot(train_df, x=train_df['date'], y=train_df['sales'])\n",
    "plt.title(f'Daily sales over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd5ef53",
   "metadata": {},
   "source": [
    "## **Bi-Variate Analysis**\n",
    "\n",
    "Exploring relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_numeric = train_df.select(pl.selectors.by_dtype(pl.Date, pl.INTEGER_DTYPES, pl.FLOAT_DTYPES))\n",
    "\n",
    "# Getting the correlation of numeric values in the train dataset\n",
    "correlation = train_df_numeric.corr()\n",
    "\n",
    "# Create the heatmap using Plotly Express\n",
    "fig = px.imshow(correlation,\n",
    "                labels=dict(color=\"Correlation\"),\n",
    "                x=correlation.columns,\n",
    "                y=correlation.columns,\n",
    "                text_auto=True,  # Automatically add text in each cell\n",
    ")\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(title=\"Correlation Heatmap\", height=700)\n",
    "\n",
    "# Set the size of text annotations\n",
    "fig.update_traces(\n",
    "    text=correlation.to_numpy(),\n",
    "    texttemplate=\"%{text:.4f}\",\n",
    "    textfont_size=12\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624aa812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships \n",
    "plt.scatter(train_df['onpromotion'], train_df['sales'])\n",
    "plt.xlabel('On Promotion')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Sales vs On Promotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a8874",
   "metadata": {},
   "source": [
    "Understand relationship on sales with external factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes to analyze relationships with external factors\n",
    "merged_df = train_df.join(holiday_events_df, on='date', how='left')\n",
    "merged_df = merged_df.join(oil_df, on='date', how='left')\n",
    "merged_df = merged_df.join(transactions_df, on=['date', 'store_nbr'], how='left')\n",
    "\n",
    "# Analyze relationships with external factors \n",
    "sales_by_holiday = merged_df.group_by('type').agg(pl.col('sales').sum())\n",
    "print(\"Sales by Holiday Type:\")\n",
    "print(sales_by_holiday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ed183",
   "metadata": {},
   "source": [
    "Decompostion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using additive model, calculate the additive results for seasons, trends and noise for the sales\n",
    "additive_results = seasonal_decompose(train_df['sales'].to_pandas(), model = 'additive', period = 365)\n",
    "\n",
    "# Update rcParams to set the figure size for the plot\n",
    "plt.rcParams.update({'figure.figsize': (15, 5)})\n",
    "\n",
    "# Plot the decomposed components (trend, seasonal, residual)\n",
    "additive_results.plot()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6375c",
   "metadata": {},
   "source": [
    "Time Series Decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series decomposition\n",
    "decomposition = seasonal_decompose(train_df['sales'].to_pandas(), model='additive', period=12)\n",
    "\n",
    "# Plot decomposition components\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(decomposition.observed)\n",
    "plt.subplot(412)\n",
    "plt.plot(decomposition.trend)\n",
    "plt.subplot(413)\n",
    "plt.plot(decomposition.seasonal)\n",
    "plt.subplot(414)\n",
    "plt.plot(decomposition.resid)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6832ad",
   "metadata": {},
   "source": [
    "Autocorrelation and Partial Autocorrelation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot autocorrelation and partial autocorrelation\n",
    "plt.figure(figsize=(12, 4))\n",
    "plot_acf(train_df['sales'], lags=40, alpha=0.05)\n",
    "plt.title('Autocorrelation')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plot_pacf(train_df['sales'], lags=40, alpha=0.05)\n",
    "plt.title('Partial Autocorrelation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87f783",
   "metadata": {},
   "source": [
    "Stationarity Test and Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6dd738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Augmented Dickey-Fuller test for stationarity\n",
    "result = adfuller(train_df['sales'])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "# Perform differencing if needed for stationarity\n",
    "if result[1] > 0.05:\n",
    "    differenced_sales = train_df['sales'].diff().dropna()\n",
    "else:\n",
    "    differenced_sales = train_df['sales']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de1b5fa",
   "metadata": {},
   "source": [
    "Seasonal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition.seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc5e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate seasonal indices\n",
    "seasonal_indices = decomposition.group_by('seasonal')(decomposition.seasonal.index.month).mean()\n",
    "\n",
    "# Plot seasonal indices\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(seasonal_indices)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Seasonal Index')\n",
    "plt.title('Seasonal Indices')\n",
    "plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c59b5b",
   "metadata": {},
   "source": [
    "Event Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30afab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge holiday/event data with sales data\n",
    "merged_df = train_df.join(holiday_events_df, on='date', how='left')\n",
    "\n",
    "# Plot sales during holidays/events\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_df['date'], merged_df['sales'], label='Sales')\n",
    "plt.plot(merged_df.filter(pl.col('type') == 'Holiday')['date'], merged_df.filter(pl.col('type') == 'Holiday')['sales'], 'ro', label='Holidays')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Sales During Holidays/Events')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e44d7",
   "metadata": {},
   "source": [
    "**Key Insights**\n",
    "1. We have identified missing dates in our date column, so we need to fill them in.\n",
    "2. Our data is not on the same scale after reviewing the summary statistics. \n",
    "3. Our train dataset has a positive skewness\n",
    "6. Noticed seasonal patterns in sales data, especially during holidays and events.\n",
    "7. Identified outliers in sales, on promotion data that may need further investigation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274bf8ca",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
